{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff",
   "metadata": {
    "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff"
   },
   "source": [
    "# Appendix E: Parameter-efficient Finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "316166b4-027a-4756-e9b4-fe88ae75dd4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tensorflow version: 2.18.0\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21532056-0ef4-4c98-82c7-e91f61c6485e",
   "metadata": {
    "id": "21532056-0ef4-4c98-82c7-e91f61c6485e"
   },
   "source": [
    "## E.1 Introduction to LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc999-3d91-4a1c-a157-9d056392e8d8",
   "metadata": {
    "id": "66edc999-3d91-4a1c-a157-9d056392e8d8"
   },
   "source": [
    "- No code in this section\n",
    "- Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters\n",
    "- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1",
   "metadata": {
    "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1"
   },
   "source": [
    "- Suppose we have a large weight matrix $W$ for a given layer\n",
    "- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training\n",
    "- In regular training and finetuning, the weight update is defined as follows:\n",
    "\n",
    "$$W_{\\text{updated}} = W + \\Delta W$$\n",
    "\n",
    "- The LoRA method proposed by [Hu et al.](https://arxiv.org/abs/2106.09685) offers a more efficient alternative to computing the weight updates $\\Delta W$ by learning an approximation of it, $\\Delta W \\approx AB$.\n",
    "- In other words, in LoRA, we have the following, where $A$ and $B$ are two small weight matrices:\n",
    "\n",
    "$$W_{\\text{updated}} = W + AB$$\n",
    "\n",
    "- The figure below illustrates these formulas for full finetuning and LoRA side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b",
   "metadata": {
    "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-1.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc",
   "metadata": {
    "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc"
   },
   "source": [
    "- If you paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier\n",
    "- That's due to the distributive law of matrix multiplication: we don't have to add the weights with the updated weights but can keep them separate\n",
    "- For instance, if $x$ is the input data, then we can write the following for regular finetuning:\n",
    "\n",
    "$$x (W+\\Delta W) = x W + x \\Delta W$$\n",
    "\n",
    "- Similarly, we can write the following for LoRA:\n",
    "\n",
    "$$x (W+A B) = x W + x A B$$\n",
    "\n",
    "- The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive\n",
    "- In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly\n",
    "- After setting up the dataset and loading the model, we will implement LoRA in the code to make these concepts less abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## E.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c64df-4431-4d27-834d-2bb38a01fc02",
   "metadata": {
    "id": "669c64df-4431-4d27-834d-2bb38a01fc02"
   },
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the dataset\n",
    "- Instead of repeating this code, one could open and run the chapter 6 notebook and then insert the LoRA code from section E.4 there\n",
    "- (The LoRA code was originally the last section of chapter 6 but was moved to the appendix due to the length of chapter 6)\n",
    "- In a similar fashion, we could also apply LoRA to the models in chapter 7 for instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "a67a7afe-b401-4463-c731-87025d20f72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {
    "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57"
   },
   "source": [
    "- As a verification step, we iterate through the data loaders and check that the batches contain 8 training examples each, where each training example consists of 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
    "outputId": "2ae34de1-dd01-4f99-d2c8-ba4dca400754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {
    "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1"
   },
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "4d19ed61-cf7a-4ec4-b822-c847dd1c5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604",
   "metadata": {
    "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604"
   },
   "source": [
    "## E.3 Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1",
   "metadata": {
    "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1"
   },
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
    "outputId": "b8c9b125-bb52-45d3-8071-fa5054dbf5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252614cd-7ce6-4908-83e6-3761f519904e",
   "metadata": {
    "id": "252614cd-7ce6-4908-83e6-3761f519904e"
   },
   "source": [
    "- To ensure that the model was loaded corrected, let's double-check that it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
    "outputId": "28ccbca5-8de9-41a0-c093-da00fcbaa91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174b31b-1ab5-4115-b01c-245369da5af3",
   "metadata": {
    "id": "8174b31b-1ab5-4115-b01c-245369da5af3"
   },
   "source": [
    "- Then, we prepare the model for classification finetuning similar to chapter 6, where we replace the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e255ce91-d73a-4854-90a4-95804928eb16",
   "metadata": {
    "id": "e255ce91-d73a-4854-90a4-95804928eb16"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02e6f057-1383-4ece-8444-0a88e71ac75d",
   "metadata": {
    "id": "02e6f057-1383-4ece-8444-0a88e71ac75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 1.2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe",
   "metadata": {
    "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe"
   },
   "source": [
    "- Lastly, let's calculate the initial classification accuracy of the non-finetuned model (we expect this to be around 50%, which means that the model is not able to distinguish between spam and non-spam messages yet reliably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
    "outputId": "74848515-5a49-4125-fecb-9f4bac23f812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b",
   "metadata": {
    "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b"
   },
   "source": [
    "## E.4 Parameter-efficient finetuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4a82-61ef-4d0a-9858-8988e844f12c",
   "metadata": {
    "id": "652a4a82-61ef-4d0a-9858-8988e844f12c"
   },
   "source": [
    "- We begin by initializing a LoRALayer that creates the matrices $A$ and $B$, along with the `alpha` scaling hyperparameter and the `rank` ($r$) hyperparameters\n",
    "- This layer can accept an input and compute the corresponding output, as illustrated in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-2.webp\" width=\"200px\">\n",
    "\n",
    "In code, this LoRA layer depicted in the figure above looks like as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ds9ywjMwvIW",
   "metadata": {
    "id": "2ds9ywjMwvIW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21faa8-0614-4257-93cd-68952193e14a",
   "metadata": {
    "id": "ad21faa8-0614-4257-93cd-68952193e14a"
   },
   "source": [
    "- In the code above, `rank` is a hyperparameter that controls the inner dimension of the matrices $A$ and $B$\n",
    "- In other words, this parameter controls the number of additional parameters introduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency\n",
    "- The second hyperparameter, `alpha`, is a scaling hyperparameter applied to the output of the low-rank adaptation\n",
    "- It essentially controls the extent to which the adapted layer's output is allowed to influence the original output of the layer being adapted\n",
    "- This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output\n",
    "- So far, the `LoRALayer` class we implemented above allows us to transform the layer inputs $x$\n",
    "- However, in LoRA, we are usually interested in replacing existing `Linear` layers so that the weight update is applied to the existing pretrained weights, as shown in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-3.webp\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f",
   "metadata": {
    "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f"
   },
   "source": [
    "- To incorporate the original `Linear` layer weights as shown in the figure above, we implement a `LinearWithLoRA` layer below that uses the previously implemented LoRALayer and can be used to replace existing `Linear` layers in a neural network, for example, the self-attention module or feed forward modules in an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "127d3a64-8359-4b21-b056-78d58cc75fe8",
   "metadata": {
    "id": "127d3a64-8359-4b21-b056-78d58cc75fe8"
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1145a90-35ff-462c-820b-15483fa5b051",
   "metadata": {
    "id": "e1145a90-35ff-462c-820b-15483fa5b051"
   },
   "source": [
    "- Note that since we initialize the weight matrix $B$ (`self.B` in `LoRALayer`) with zero values in the LoRA layer, the matrix multiplication between $A$ and $B$ results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 to the original weights does not modify them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d",
   "metadata": {
    "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d"
   },
   "source": [
    "- To try LoRA on the GPT model we defined earlier, we define a `replace_linear_with_lora` function to replace all `Linear` layers in the model with the new `LinearWithLoRA` layers\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-4.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "WlQZ8ygqzN_g",
   "metadata": {
    "id": "WlQZ8ygqzN_g"
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2",
   "metadata": {
    "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2"
   },
   "source": [
    "- We then freeze the original model parameter and use the `replace_linear_with_lora` to replace the said `Linear` layers using the code below\n",
    "- This will replace the `Linear` layers in the LLM with `LinearWithLoRA` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
    "outputId": "fd4c208f-854a-4701-d9d3-9d73af733364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mLk_fPq0yz_u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLk_fPq0yz_u",
    "outputId": "0a93b8fc-05d7-4ace-ee47-e2fc6bdd7d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9",
   "metadata": {
    "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9"
   },
   "source": [
    "- As we can see, we reduced the number of trainable parameters by almost 50x when using LoRA\n",
    "- Let's now double-check whether the layers have been modified as intended by printing the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
    "outputId": "acff8eca-3775-45a2-b62d-032a986ef037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55",
   "metadata": {
    "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55"
   },
   "source": [
    "- Based on the model architecture above, we can see that the model now contains our new `LinearWithLoRA` layers\n",
    "- Also, since we initialized matrix $B$ with 0's, we expect the initial model performance to be unchanged compared to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "DAlrb_I00VEU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAlrb_I00VEU",
    "outputId": "3da44ac4-230b-4358-d996-30b63f0d962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101",
   "metadata": {
    "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101"
   },
   "source": [
    "- Let's now get to the interesting part and finetune the model by reusing the training function from chapter 6\n",
    "- The training takes about 15 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "wCParRvr0eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCParRvr0eff",
    "outputId": "ce910a9c-ee89-48bb-bfa6-49c6aee1e450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.009, Val loss 0.049\n",
      "Ep 2 (Step 000250): Train loss 0.024, Val loss 0.184\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.099, Val loss 0.037\n",
      "Ep 3 (Step 000350): Train loss 0.027, Val loss 0.222\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.004, Val loss 0.180\n",
      "Ep 4 (Step 000450): Train loss 0.059, Val loss 0.251\n",
      "Ep 4 (Step 000500): Train loss 0.012, Val loss 0.200\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.006, Val loss 0.169\n",
      "Ep 5 (Step 000600): Train loss 0.001, Val loss 0.236\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 2.75 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c",
   "metadata": {
    "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c"
   },
   "source": [
    "- Finally, let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bawWGijA0iF3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "bawWGijA0iF3",
    "outputId": "af70782a-d605-4376-fa6c-d33b38979cfa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa074723-e3f7-4f7e-a267-855531a037dc",
   "metadata": {
    "id": "aa074723-e3f7-4f7e-a267-855531a037dc"
   },
   "source": [
    "- Note that we previously calculated the accuracy values on 5 batches only via the `eval_iter=5` setting; below, we calculate the accuracies on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1D2awlEq0gZi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D2awlEq0gZi",
    "outputId": "d603eda1-d912-43eb-ec9c-af6a622510a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.00%\n",
      "Validation accuracy: 97.99%\n",
      "Test accuracy: 97.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d",
   "metadata": {
    "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d"
   },
   "source": [
    "- As we can see based on the relatively high accuracy values above, the LoRA finetuning was successful"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "build-a-llm-from-stratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
